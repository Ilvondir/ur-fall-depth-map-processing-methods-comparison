{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8785065c",
   "metadata": {},
   "source": [
    "# Basic depth map processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b86abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, ConfusionMatrixDisplay, f1_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def seed_everything(seed=4242):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9724cceb",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_to_normals_3d(depth):\n",
    "    h, w = depth.shape\n",
    "    depth = depth.astype(np.float32)\n",
    "\n",
    "    fx = w\n",
    "    fy = w\n",
    "    cx = w / 2\n",
    "    cy = h / 2\n",
    "\n",
    "    u, v = np.meshgrid(np.arange(w), np.arange(h))\n",
    "\n",
    "    x = (u - cx) * depth / fx\n",
    "    y = (v - cy) * depth / fy\n",
    "    z = depth\n",
    "    \n",
    "    points = np.stack((x, y, z), axis=-1)\n",
    "    dy, dx = np.gradient(points, axis=(0, 1))\n",
    "\n",
    "    normals = np.cross(dx, dy)\n",
    "\n",
    "    norm = np.linalg.norm(normals, axis=2, keepdims=True)\n",
    "    normals /= (norm + 1e-8)\n",
    "\n",
    "    normals_rgb = ((normals + 1) * 0.5 * 255).astype(np.uint8)\n",
    "    return normals_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'fall' # fall, adl\n",
    "sequence_number = random.randint(0, 40 if label == 'adl' else 30) + 1\n",
    "selected_video = Path(f'../datasets/{label}/sequence-{sequence_number:02}')\n",
    "\n",
    "frames = sorted(selected_video.glob('*.png'))\n",
    "\n",
    "for frame_path in frames:\n",
    "    depth = cv2.imread(str(frame_path), cv2.IMREAD_GRAYSCALE)\n",
    "    normals = depth_to_normals_3d(depth)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(depth, cmap='gray')\n",
    "    plt.title(\"Depth\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(normals)\n",
    "    plt.title(\"Surface normals\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cbbcbf",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf67a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = Path(\"../datasets\")\n",
    "# out_root = Path(\"../data_normals\")\n",
    "# out_root.mkdir(exist_ok=True)\n",
    "\n",
    "# for cls in [\"adl\", \"fall\"]:\n",
    "#     for seq in (root / cls).iterdir():\n",
    "#         out_seq = out_root / cls / seq.name\n",
    "#         out_seq.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         for f in seq.glob(\"*.png\"):\n",
    "#             depth = cv2.imread(str(f), cv2.IMREAD_GRAYSCALE)\n",
    "#             normals = depth_to_normals_3d(depth)\n",
    "\n",
    "#             np.save(out_seq / f\"{f.stem}.npy\", normals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ccb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None, number_of_frames=64):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.number_of_frames = number_of_frames\n",
    "\n",
    "        classes = ['adl', 'fall']\n",
    "        for label_idx, cls in enumerate(classes):\n",
    "            cls_path = Path(root_dir) / cls\n",
    "            for seq_folder in cls_path.iterdir():\n",
    "                frames = sorted(seq_folder.glob(\"*.npy\"))\n",
    "                if frames:\n",
    "                    self.samples.append((frames, label_idx))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths, label = self.samples[idx]\n",
    "\n",
    "        if len(frame_paths) >= self.number_of_frames:\n",
    "            frame_paths = frame_paths[-self.number_of_frames:]\n",
    "        else:\n",
    "            frame_paths = [frame_paths[0]] * (self.number_of_frames - len(frame_paths)) + frame_paths\n",
    "\n",
    "        imgs = []\n",
    "        for f in frame_paths:\n",
    "            arr = np.load(f).astype(np.float32)          # (H, W, 3)\n",
    "            img = Image.fromarray((arr * 255).astype(np.uint8)) # (3, H, W)\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            imgs.append(img)\n",
    "\n",
    "        video = torch.stack(imgs)  # (T, 3, H, W)\n",
    "        return video, label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "root_dir = Path('../data_normals')\n",
    "\n",
    "dataset = VideoDataset(root_dir, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a1239",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25093bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, cnn_model='resnet18', hidden_size=256, num_classes=2, pretrained=True):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        self.cnn = models.resnet18(pretrained=pretrained)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])  # output: (B, 512, 1, 1)\n",
    "\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.feature_dim = 512\n",
    "        self.lstm = nn.LSTM(input_size=self.feature_dim, hidden_size=hidden_size, \n",
    "                          num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, C, H, W)\n",
    "        B = batch_size\n",
    "        T = num_frames\n",
    "        C = channels\n",
    "        H = height\n",
    "        W = width\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.size()\n",
    "\n",
    "        cnn_features = []\n",
    "        for t in range(T):\n",
    "            frame = x[:, t, :, :, :]        # (B, C, H, W)\n",
    "            feat = self.cnn(frame)          # (B, 512, 1, 1)\n",
    "            feat = feat.view(B, -1)         # (B, 512)\n",
    "            cnn_features.append(feat)\n",
    "        \n",
    "\n",
    "        cnn_features = torch.stack(cnn_features, dim=1) # (B, T, feature_dim)\n",
    "    \n",
    "        lstm_out, _ = self.lstm(cnn_features) \n",
    "        last_time_step = lstm_out[:, -1, :] # (B, hidden_size)\n",
    "\n",
    "        out = self.fc(last_time_step) # (B, num_classes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50766ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_LSTM().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# summary(model, input_size=(2, 256, 3, 224, 224), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c2290",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320166b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 4\n",
    "num_folds = 10\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "seed_everything(42)\n",
    "\n",
    "indices = np.arange(len(dataset))\n",
    "labels = np.array([dataset[i][1] for i in indices])\n",
    "\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=num_folds,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "checkpoint_dir = Path('../models/surfaces_normals')\n",
    "checkpoint_dir.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c5efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results = []\n",
    "\n",
    "full_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        generator=g\n",
    "    )\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(indices, labels)):\n",
    "    print(f\"\\n========== FOLD {fold+1}/{num_folds} ==========\")\n",
    "\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        generator=g\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    best_val_loss = 1e10\n",
    "    best_model_path = checkpoint_dir / f'best_model_fold_{fold}.pt'\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ===================== TRAIN =====================\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for videos, y in train_loader:\n",
    "            videos = videos.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * videos.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        train_loss /= len(train_dataset)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # ===================== VAL =====================\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for videos, y in val_loader:\n",
    "                videos = videos.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                outputs = model(videos)\n",
    "                loss = criterion(outputs, y)\n",
    "\n",
    "                val_loss += loss.item() * videos.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        val_loss /= len(val_dataset)\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "            f\"TRAIN Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | \"\n",
    "            f\"VAL Loss: {val_loss:.4f} Acc: {val_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    # ===================== EVAL BEST MODEL =====================\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for videos, y in full_loader:\n",
    "            videos = videos.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(videos)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    fold_metrics = {\n",
    "        'acc': accuracy_score(all_labels, all_preds),\n",
    "        'prec': precision_score(all_labels, all_preds, average='binary'),\n",
    "        'rec': recall_score(all_labels, all_preds, average='binary'),\n",
    "        'f1': f1_score(all_labels, all_preds, average='binary'),\n",
    "    }\n",
    "\n",
    "    fold_results.append(fold_metrics)\n",
    "\n",
    "    print(\"Fold results:\", fold_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1da8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n========== CV RESULTS ==========\")\n",
    "\n",
    "for metric in fold_results[0].keys():\n",
    "    values = [f[metric] for f in fold_results]\n",
    "    print(\n",
    "        f\"{metric.upper()}: \"\n",
    "        f\"{np.mean(values)*100:.2f}% std: {np.std(values)*100:.2f}%\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ur-fall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
